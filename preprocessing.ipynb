{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\max\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['books', 'are', 'on', 'the', 'table']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"books are on the table\"\n",
    "tokens = word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StopWords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\max\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Machine', 'learning', 'cool']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "sentence = \"Machine learning is so cool\"\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(sentence)\n",
    "filtered_sentence = [w for w in word_tokens if w not in stopwords]\n",
    "\n",
    "filtered_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machin', 'learn', 'feel', 'so', 'cool']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "sentence = \"Machine learning feels so cool\"\n",
    "sentence_tokens = word_tokenize(sentence)\n",
    "\n",
    "stemmed_sentence = [ps.stem(s) for s in sentence_tokens]\n",
    "stemmed_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatizing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\max\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Machine', 'learning', 'feel', 'so', 'cool']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmatized_sentence = [wnl.lemmatize(s) for s in sentence_tokens]\n",
    "lemmatized_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POS Tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\max\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tokens = word_tokenize(\"And now for something completely different\")\n",
    "tokens_tags = pos_tag(tokens)\n",
    "tokens_tags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brown Automatic Tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\max\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4623\n",
      "4623\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury']\n",
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "\n",
    "brown_sents = brown.sents(categories='news')\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "\n",
    "print(len(brown_sents))\n",
    "print(len(brown_tagged_sents))\n",
    "\n",
    "print(brown_sents[0][:5])\n",
    "print(brown_tagged_sents[0][:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Automatic Tagging (Default Tagger)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n",
    "highest_freq_tag = nltk.FreqDist(tags).max()\n",
    "highest_freq_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'NN'),\n",
       " ('do', 'NN'),\n",
       " ('not', 'NN'),\n",
       " ('like', 'NN'),\n",
       " ('green', 'NN'),\n",
       " ('eggs', 'NN'),\n",
       " ('and', 'NN'),\n",
       " ('ham', 'NN'),\n",
       " (',', 'NN'),\n",
       " ('I', 'NN'),\n",
       " ('do', 'NN'),\n",
       " ('not', 'NN'),\n",
       " ('like', 'NN'),\n",
       " ('them', 'NN'),\n",
       " ('Sam', 'NN'),\n",
       " ('I', 'NN'),\n",
       " ('am', 'NN'),\n",
       " ('!', 'NN')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"I do not like green eggs and ham, I do not like them Sam I am!\"\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "default_tagger = nltk.DefaultTagger(highest_freq_tag)\n",
    "default_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13089484257215028"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_tagger.accuracy(brown_tagged_sents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Automatic Tagging (Regular Expression Tagger)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "            (r'.*ing$', 'VBG'),  #gerunds\n",
    "            (r'.*ed$', 'VBD'),   #simple past\n",
    "            (r'.*es$', 'VBZ'),   #3rd singular present\n",
    "            (r'.*ould$', 'MD'),  #modals\n",
    "            (r'.*\\'s$', 'NN$'),  #possessive nouns\n",
    "            (r'.*s$', 'NNS'),    #plural nouns\n",
    "            (r'.*^-?[0-9]+(\\.[0-9]+)?$', 'CD'),  #cardinal numbers\n",
    "            (r'.*$', 'NN')\n",
    "        ]      #nouns (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('``', 'NN'),\n",
       " ('Only', 'NN'),\n",
       " ('a', 'NN'),\n",
       " ('relative', 'NN'),\n",
       " ('handful', 'NN'),\n",
       " ('of', 'NN'),\n",
       " ('such', 'NN'),\n",
       " ('reports', 'NNS'),\n",
       " ('was', 'NNS'),\n",
       " ('received', 'VBD')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "regexp_tagger.tag(brown_sents[3][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20186168625812995"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tagger.accuracy(brown_tagged_sents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Automatic Tagging (N-Gram Tagging)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Various', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('apartments', 'NNS'),\n",
       " ('are', 'BER'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('terrace', 'NN'),\n",
       " ('type', 'NN'),\n",
       " (',', ','),\n",
       " ('being', 'BEG'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('ground', 'NN'),\n",
       " ('floor', 'NN'),\n",
       " ('so', 'QL'),\n",
       " ('that', 'CS'),\n",
       " ('entrance', 'NN'),\n",
       " ('is', 'BEZ'),\n",
       " ('direct', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
    "unigram_tagger.tag(brown_sents[2007])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9349006503968017"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger.accuracy(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('But', 'CC'), ('in', 'IN'), ('all', 'ABN'), ('its', 'PP$'), ('175', 'CD'), ('years', 'NNS'), (',', ','), ('not', '*'), ('a', 'AT'), ('single', 'AP'), ('Negro', 'NP'), ('student', 'NN'), ('has', 'HVZ'), ('entered', 'VBN'), ('its', 'PP$'), ('classrooms', 'NNS'), ('.', '.')], [('Last', 'AP'), ('week', 'NN'), ('Federal', 'JJ-TL'), ('District', 'NN-TL'), ('Judge', 'NN-TL'), ('William', 'NP'), ('A.', 'NP'), ('Bootle', 'NP'), ('ordered', 'VBD'), ('the', 'AT'), ('university', 'NN'), ('to', 'TO'), ('admit', 'VB'), ('immediately', 'RB'), ('a', 'AT'), ('``', '``'), ('qualified', 'VBN'), (\"''\", \"''\"), ('Negro', 'NP'), ('boy', 'NN'), ('and', 'CC'), ('girl', 'NN'), ('.', '.')], ...]\n"
     ]
    }
   ],
   "source": [
    "size = int(len(brown_sents) * 0.9)\n",
    "training_set = brown_tagged_sents[:size]\n",
    "testing_set = brown_tagged_sents[size:]\n",
    "print(testing_set)  \n",
    "unigram_tagger = nltk.UnigramTagger(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9353630649241612"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger.accuracy(training_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Automatic Tagging (N-Gram Tagging)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Various', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('apartments', 'NNS'),\n",
       " ('are', 'BER'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('terrace', 'NN'),\n",
       " ('type', 'NN'),\n",
       " (',', ','),\n",
       " ('being', 'BEG'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('ground', 'NN'),\n",
       " ('floor', 'NN'),\n",
       " ('so', 'CS'),\n",
       " ('that', 'CS'),\n",
       " ('entrance', 'NN'),\n",
       " ('is', 'BEZ'),\n",
       " ('direct', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(training_set)\n",
    "bigram_tagger.tag(brown_sents[2007])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('population', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('Congo', 'NP'),\n",
       " ('is', 'BEZ'),\n",
       " ('13.5', None),\n",
       " ('million', None),\n",
       " (',', None),\n",
       " ('divided', None),\n",
       " ('into', None),\n",
       " ('at', None),\n",
       " ('least', None),\n",
       " ('seven', None),\n",
       " ('major', None),\n",
       " ('``', None),\n",
       " ('culture', None),\n",
       " ('clusters', None),\n",
       " (\"''\", None),\n",
       " ('and', None),\n",
       " ('innumerable', None),\n",
       " ('tribes', None),\n",
       " ('speaking', None),\n",
       " ('400', None),\n",
       " ('separate', None),\n",
       " ('dialects', None),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_sent = brown_sents[4203]\n",
    "bigram_tagger.tag(unseen_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10206319146815508"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger.accuracy(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8452108043456593"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(training_set, backoff=t0)\n",
    "t2 = nltk.BigramTagger(training_set, backoff=t1)\n",
    "\n",
    "t2.accuracy(testing_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Automatic Tagging (Brill's Tagger)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\max\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tagged data from treebank... \n",
      "Read testing data (200 sents/5251 wds)\n",
      "Read training data (800 sents/19933 wds)\n",
      "Read baseline data (800 sents/19933 wds) [reused the training set]\n",
      "Trained baseline tagger\n",
      "    Accuracy on test set: 0.8358\n",
      "Training tbl tagger...\n",
      "TBL train (fast) (seqs: 800; tokens: 19933; tpls: 24; min score: 3; min acc: None)\n",
      "Finding initial useful rules...\n",
      "    Found 12799 useful rules.\n",
      "\n",
      "           B      |\n",
      "   S   F   r   O  |        Score = Fixed - Broken\n",
      "   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n",
      "   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n",
      "   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n",
      "   e   d   n   r  |  e\n",
      "------------------+-------------------------------------------------------\n",
      "  23  23   0   0  | POS->VBZ if Pos:PRP@[-2,-1]\n",
      "  18  19   1   0  | NN->VB if Pos:-NONE-@[-2] & Pos:TO@[-1]\n",
      "  14  14   0   0  | VBP->VB if Pos:MD@[-2,-1]\n",
      "  12  12   0   0  | VBP->VB if Pos:TO@[-1]\n",
      "  11  11   0   0  | VBD->VBN if Pos:VBD@[-1]\n",
      "  11  11   0   0  | IN->WDT if Pos:-NONE-@[1] & Pos:VBP@[2]\n",
      "  10  11   1   0  | VBN->VBD if Pos:PRP@[-1]\n",
      "   9  10   1   0  | VBD->VBN if Pos:VBZ@[-1]\n",
      "   8   8   0   0  | NN->VB if Pos:MD@[-1]\n",
      "   7   7   0   1  | VB->NN if Pos:DT@[-1]\n",
      "   7   7   0   0  | VB->VBP if Pos:PRP@[-1]\n",
      "   7   7   0   0  | IN->WDT if Pos:-NONE-@[1] & Pos:VBZ@[2]\n",
      "   7   8   1   0  | IN->RB if Word:as@[2]\n",
      "   6   6   0   0  | VBD->VBN if Pos:VBP@[-2,-1]\n",
      "   6   6   0   1  | IN->WDT if Pos:-NONE-@[1] & Pos:VBD@[2]\n",
      "   5   5   0   0  | POS->VBZ if Pos:-NONE-@[-1]\n",
      "   5   5   0   0  | VB->VBP if Pos:NNS@[-1]\n",
      "   5   5   0   0  | VBD->VBN if Word:be@[-2,-1]\n",
      "   4   4   0   0  | POS->VBZ if Pos:``@[-2]\n",
      "   4   4   0   0  | VBP->VB if Pos:VBD@[-2,-1]\n",
      "   4   6   2   3  | RP->RB if Pos:CD@[1,2]\n",
      "   4   4   0   0  | RB->JJ if Pos:DT@[-1] & Pos:NN@[1]\n",
      "   4   4   0   0  | NN->VBP if Pos:NNS@[-2] & Pos:RB@[-1]\n",
      "   4   5   1   0  | VBN->VBD if Pos:NNP@[-2] & Pos:NNP@[-1]\n",
      "   4   4   0   0  | IN->WDT if Pos:-NONE-@[1] & Pos:MD@[2]\n",
      "   4   8   4   0  | VBD->VBN if Word:*@[1]\n",
      "   4   4   0   0  | JJS->RBS if Word:most@[0] & Word:the@[-1] & Pos:DT@[-1]\n",
      "   3   3   0   0  | VBD->VBN if Pos:VBN@[-1]\n",
      "   3   4   1   0  | VBN->VB if Pos:TO@[-1]\n",
      "   3   4   1   1  | IN->RB if Pos:.@[1]\n",
      "   3   3   0   0  | JJ->RB if Pos:VBD@[1]\n",
      "   3   3   0   0  | PRP$->PRP if Pos:TO@[1]\n",
      "   3   3   0   0  | NN->VBP if Pos:NNS@[-1] & Pos:DT@[1]\n",
      "   3   3   0   0  | VBP->VB if Word:n't@[-2,-1]\n",
      "Trained tbl tagger in 2.91 seconds\n",
      "    Accuracy on test set: 0.8564\n",
      "Tagging the test data\n"
     ]
    }
   ],
   "source": [
    "from nltk.tbl import demo as brill_demo\n",
    "nltk.download('treebank')\n",
    "brill_demo.demo()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Named Entity Recognition (NER)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tag import StanfordNERTagger\n",
    "# st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') \n",
    "# st.tag('Mustafa Walid is in the University of MSA. He majors in computer science and wishes to work in google')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ONE HOT ENCODING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cold' 'cold' 'warm' 'cold' 'hot' 'hot' 'warm' 'cold' 'warm' 'hot']\n",
      "[0 0 2 0 1 1 2 0 2 1]\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from numpy import array\n",
    "\n",
    "#define example\n",
    "data = ['cold','cold','warm','cold','hot','hot','warm','cold','warm','hot']\n",
    "values = array(data)\n",
    "print(values)\n",
    "#integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "\n",
    "#binary encode\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(integer_encoded)\n",
    "print(one_hot_encoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count Vectorizer & TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "train = ['The sky is blue.', 'The sun is bright.']\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer='word', stop_words='english')\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "\n",
    "count_wm = count_vectorizer.fit_transform(train)\n",
    "tfidf_wm = tfidf_vectorizer.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0],\n",
       "       [0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_wm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['blue', 'bright', 'sky', 'sun'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens = count_vectorizer.get_feature_names_out()\n",
    "tfidf_tokens = tfidf_vectorizer.get_feature_names_out()\n",
    "count_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blue</th>\n",
       "      <th>bright</th>\n",
       "      <th>sky</th>\n",
       "      <th>sun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      blue  bright  sky  sun\n",
       "Doc1     1       0    1    0\n",
       "Doc2     0       1    0    1"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count_vect = pd.DataFrame(data=count_wm.toarray(), index=['Doc1', 'Doc2'], columns=count_tokens)\n",
    "df_count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blue</th>\n",
       "      <th>bright</th>\n",
       "      <th>sky</th>\n",
       "      <th>sun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          blue    bright       sky       sun\n",
       "Doc1  0.707107  0.000000  0.707107  0.000000\n",
       "Doc2  0.000000  0.707107  0.000000  0.707107"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf_vect = pd.DataFrame(data=tfidf_wm.toarray(), index=['Doc1', 'Doc2'], columns=tfidf_tokens)\n",
    "df_tfidf_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer\n",
      "\n",
      "      blue  bright  sky  sun\n",
      "Doc1     1       0    1    0\n",
      "Doc2     0       1    0    1\n",
      "\n",
      "Tf-IDF Vectorizer\n",
      "\n",
      "          blue    bright       sky       sun\n",
      "Doc1  0.707107  0.000000  0.707107  0.000000\n",
      "Doc2  0.000000  0.707107  0.000000  0.707107\n"
     ]
    }
   ],
   "source": [
    "print(\"Count Vectorizer\\n\")\n",
    "print(df_count_vect)\n",
    "print(\"\\nTf-IDF Vectorizer\\n\")\n",
    "print(df_tfidf_vect)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N-Gram Reperesentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>changed world</th>\n",
       "      <th>love nlp</th>\n",
       "      <th>nlp changed</th>\n",
       "      <th>nlp cool</th>\n",
       "      <th>nlp future</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      changed world  love nlp  nlp changed  nlp cool  nlp future\n",
       "Doc1              1         0            1         0           0\n",
       "Doc2              0         1            0         0           0\n",
       "Doc3              0         0            0         1           0\n",
       "Doc4              0         0            0         0           1"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['NLP has changed the world', 'I love NLP', 'NLP is cool', 'NLP is future']\n",
    "count_vectorizer = CountVectorizer(ngram_range=(2,2), stop_words='english')\n",
    "\n",
    "#Bigram Count Vectorizer\n",
    "count_wm = count_vectorizer.fit_transform(text)\n",
    "count_tokens = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "df_count_vect = pd.DataFrame(data=count_wm.toarray(), index=['Doc1', 'Doc2','Doc3','Doc4'], columns=count_tokens)\n",
    "df_count_vect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fdc5a2d8dec475f76a8cf302e325139a2d43c66fc61cd7bb8efa61dee969b4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
